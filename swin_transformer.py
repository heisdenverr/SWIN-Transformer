# -*- coding: utf-8 -*-
"""Swin-Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTUVsRe9sD7jdYHithOkXctx12rB3pHX
"""

import torch
from torch import nn

# Continue with regular imports
import matplotlib.pyplot as plt
import torch
import torchvision

from torch import nn
from torchvision import transforms

# Try to get torchinfo, install it if it doesn't work
try:
    from torchinfo import summary
except:
    print("[INFO] Couldn't find torchinfo... installing it.")
    !pip install -q torchinfo
    from torchinfo import summary

# Try to import the going_modular directory, download it from GitHub if it doesn't work
try:
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves
except:
    # Get the going_modular scripts
    print("[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.")
    !git clone https://github.com/mrdbourke/pytorch-deep-learning
    !mv pytorch-deep-learning/going_modular .
    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script
    !rm -rf pytorch-deep-learning
    from going_modular.going_modular import data_setup, engine
    from helper_functions import download_data, set_seeds, plot_loss_curves

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Download pizza, steak, sushi images from GitHub
image_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip",
                           destination="pizza_steak_sushi")
image_path

# Setup directory paths to train and test images
train_dir = image_path / "train"
test_dir = image_path / "test"

!pip install torchinfo

class PatchEmbedding(nn.Module):
    def __init__(self, in_channels=3, embed_dim=96, patch_size=4):
        super().__init__()
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)  # Shape: [batch_size, embed_dim, h//patch_size, w//patch_size]
        return x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]

class PatchPartitioning(nn.Module):
  def __init__(self, in_channels=3, embed_dim=96, patch_size=4):
    super().__init__()
    self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

  def forward(self, x):
    x = self.proj(x)
    return x.flatten(2).transpose(1, 2)

class PatchMerging(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(PatchMerging, self).__init__()
    self.reduction = nn.Linear(4*input_dim, output_dim)

  def forward(self, x, H, W):
    B, L, C = x.shape
    assert L == H*W ,f"Input features has wrong size"
    x = x.reshape(B, H, W, C)
    x0 = x[:, 0::2, 0::2, :] # TOP-LEFT
    x1 = x[:, 1::2, 0::2, :] # BOTTOM-LEFT
    x2 = x[:, 0::2, 1::2, :] # TOP-RIGHT
    x3 = x[:, 1::2, 1::2, :] # BOTTOM-RIGHT
    x = torch.cat([x0, x1, x2, x3], -1) # concatenate along feature dimension
    x = x.reshape(B, -1, 4 * C)

    x = self.reduction(x) # [B, H/2 * W/2, output_dim]
    return x

class WindowAttention(nn.Module):
  def __init__(self,dim, window_size, num_heads):
    super(WindowAttention, self).__init__()
    self.dim = dim
    self.window_size = window_size
    self.num_heads = num_heads
    self.attention = nn.MultiheadAttention(dim, num_heads)

  def forward(self, x):
    B, N, C = x.shape
    x = x.reshape(-1, self.window_size*self.window_size, C) # [num_windows*B, window_size*window_size, C]
    x = x.transpose(0, 1)
    attn_output, _ = self.attention(x, x, x)
    attn_output = attn_output.transpose(0, 1).reshape(B, -1, C)
    return attn_output

class SwinTransformerBlock(nn.Module):
  def __init__(self, dim, input_resolution, num_heads, window_size, shift_size=0):
    super(SwinTransformerBlock, self).__init__()
    self.window_size = window_size
    self.num_heads = num_heads
    self.shift_size = shift_size
    self.attention = WindowAttention(dim, window_size, num_heads)

  def forward(self, x, H, W):
    if self.shift_size > 0:
      x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))

    x = self.attention(x)

    if self.shift_size > 0:
      x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))

    return x

class SwinTransformer(nn.Module):
  def __init__(self, image_size=224, patch_size=4, in_channels=3, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24]):
    super(SwinTransformer, self).__init__()
    self.patch_embed = PatchPartitioning(in_channels, embed_dim, patch_size)
    self.stages = nn.ModuleList()
    self.patch_merging = nn.ModuleList()

    for i in range(len(depths)):

      stage = nn.ModuleList([SwinTransformerBlock(dim=embed_dim*2**i, input_resolution=image_size // (2**(i+1)),
                                                  num_heads=num_heads[i], window_size=7, shift_size=0 if j & 2 ==0 else 3)
              for j in range(depths[i])])
      self.stages.append(stage)
      if i < len(depths) - 1:
        self.patch_merging.append(PatchMerging(embed_dim *2**i, embed_dim * 2**(i+1)))

    self.classifier = nn.Sequential(
        nn.Linear(in_features=768, out_features=len(class_names))
    )

  def forward(self, x):
    x = self.patch_embed(x)
    H, W = 56, 56

    for i, stage in enumerate(self.stages):
      for block in stage:
        x = block(x, H, W)

      if i < len(self.patch_merging):
        x = self.patch_merging[i](x, H, W)
        H, W = H // 2, W // 2

    x = x.flatten(2)
    x = x.mean(dim=1)

    return self.classifier(x)

model = SwinTransformer()

model

model.patch_merging

# Create a simplified version of the model to test specific components
simplified_model = model.patch_merging  # Or use a specific stage for testing

# Check the summary with the simplified component
summary(simplified_model)

# Print a summary of our custom ViT model using torchinfo (uncomment for actual output)
summary(model=model,
        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)
        # col_names=["input_size"], # uncomment for smaller output
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"]
)

# Create image size (from Table 3 in the ViT paper)
IMG_SIZE = 224

# Create transform pipeline manually
manual_transforms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
])
print(f"Manually created transforms: {manual_transforms}")

# Set the batch size
BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small

# Create data loaders
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=manual_transforms, # use manually created transforms
    batch_size=BATCH_SIZE
)

train_dataloader, test_dataloader, class_names

from going_modular.going_modular import engine

# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper
optimizer = torch.optim.Adam(params=model.parameters(),
                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k
                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)
                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k

# Setup the loss function for multi-class classification
loss_fn = torch.nn.CrossEntropyLoss()

# Set the seeds
set_seeds()

model.to(device)
# Train the model and save the training results to a dictionary

for n in range(7):

  print(f"Epochs-----")
  train_loss = 0
  model.train()
  for i, (X, y) in enumerate(train_dataloader):
    X, y = X.to(device), y.to(device)
    optimizer.zero_grad()
    predicted = model(X)
    loss = loss_fn(predicted, y)
    train_loss += loss
    loss.backward()
    optimizer.step()

  print(f"Epoch {n + 1}, Loss: {train_loss / len(train_dataloader):.3f}")

results = engine.train(model=model,
                       train_dataloader=train_dataloader,
                       test_dataloader=test_dataloader,
                       optimizer=optimizer,
                       loss_fn=loss_fn,
                       epochs=10,
                       device=device)

